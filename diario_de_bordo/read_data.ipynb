{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ETL to InfoCorridasDia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LeituraParquet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, count, col, when, desc, min, max, sum, round, to_date, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "parquet_path = \"/Users/andrebezerra/Desktop/Dev/DesafiosCodeElevate/diario_de_bordo/transportes.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet\n",
    "df_spark = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|DATA_INICIO|QT_CORR|QT_CORR_NEG|QT_CORR_PESS|VL_MAX_DIST|VL_MIN_DIST|VL_AVG_DIST|QT_CORR_REUNI|QT_CORR_NAO_REUNI|\n",
      "+-----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "| 2016-12-31|      5|          5|           0|        482|          7|      150.8|            2|                3|\n",
      "| 2016-12-30|      5|          5|           0|         46|          8|       31.4|            1|                4|\n",
      "| 2016-12-29|     13|         13|           0|        129|          3|      55.54|            2|               10|\n",
      "| 2016-12-28|      7|          7|           0|        104|          2|       61.0|            0|                7|\n",
      "| 2016-12-27|      6|          6|           0|         79|          5|       37.5|            1|                5|\n",
      "| 2016-12-26|      5|          5|           0|         79|         32|       57.6|            1|                4|\n",
      "| 2016-12-25|      3|          3|           0|         23|          6|      17.33|            0|                3|\n",
      "| 2016-12-24|      8|          8|           0|        107|          6|      40.88|            0|                8|\n",
      "| 2016-12-23|      5|          5|           0|         96|          3|       49.0|            3|                2|\n",
      "| 2016-12-22|      9|          9|           0|        323|         14|     103.89|            4|                5|\n",
      "| 2016-12-21|     10|         10|           0|        162|          2|       59.2|            5|                5|\n",
      "| 2016-12-20|      8|          8|           0|        194|         12|       56.0|            1|                6|\n",
      "| 2016-12-19|     11|         11|           0|        102|          7|       45.0|            1|                5|\n",
      "| 2016-12-18|      3|          3|           0|        102|         49|       81.0|            0|                2|\n",
      "| 2016-12-17|      2|          2|           0|         53|         48|       50.5|            0|                2|\n",
      "+-----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remover a parte da hora da DATA_INICIO\n",
    "df_spark = df_spark.withColumn(\"DATA_INICIO\", substring(\"DATA_INICIO\", 1, 10))\n",
    "\n",
    "# Convertendo para formato yyyy-MM-dd\n",
    "df_spark = df_spark.withColumn(\"DATA_INICIO\", to_date(col(\"DATA_INICIO\"), \"MM-dd-yyyy\"))\n",
    "\n",
    "# Agregar os dados\n",
    "df_resultado = df_spark.groupBy(\"DATA_INICIO\").agg(\n",
    "    count(col(\"LOCAL_INICIO\")).alias(\"QT_CORR\"),        # total travels\n",
    "    count(when(col(\"CATEGORIA\") == \"Negocio\", True)).alias(\"QT_CORR_NEG\"),      # Count travels like 'Negocio'\n",
    "    count(when(col(\"CATEGORIA\") == \"Pessoal\", True)).alias(\"QT_CORR_PESS\"),     # Count travels like 'Pessoal'\n",
    "    max(col(\"DISTANCIA\")).alias(\"VL_MAX_DIST\"),     # longest distance of the day\n",
    "    min(col(\"DISTANCIA\")).alias(\"VL_MIN_DIST\"),     # shortest distance of the day\n",
    "    round(sum(col(\"DISTANCIA\")) / count(col(\"DATA_INICIO\")), 2).alias(\"VL_AVG_DIST\"),       # average total distance\n",
    "    count(when(col(\"PROPOSITO\").isin(\"Reuni達o\"), True)).alias(\"QT_CORR_REUNI\"),     # Travels for the purpose of \"Reuni達o\".\n",
    "    count(when((col(\"PROPOSITO\").isNotNull()) & (~col(\"PROPOSITO\").isin(\"Reuni達o\")), True)).alias(\"QT_CORR_NAO_REUNI\")      # Travels with a stated purpose other than \"Reuni達o\".  \n",
    ").orderBy(col(\"DATA_INICIO\").desc())\n",
    "df_resultado.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
